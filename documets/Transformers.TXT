Intro
0:00
Chad GPT is powered by a model called GPT which is based on a deep learning
0:05
architecture called Transformers Transformers is the reason behind modern day AI boom as an AI Enthusiast when you
0:12
start learning Transformers you will come across this complex diagram which will start giving you a headache
0:18
immediately my goal for today's video is to explain you Transformers in a most
0:23
simplified and intuitive manner we need to cover many different topics so this
0:28
is going to be a long video attention and patience is all I need from you today when you type in a sentence in
0:35
Gmail it tries to predict next word or next set of words this is possible because of a machine learning model
0:41
called language model Google for example has this popular language model called
0:46
bird which is powering hundreds of AI applications throughout the world GPT
0:52
which is a model behind chat GPT is a large language model the reason it is called large language model is because
0:59
it has billions of parameters it is much more capable and Powerful compared to
1:05
bird and it is trained on humongous amount of data fundamentally though it
1:10
is also doing the same thing which is when you type in a question in chat GPT it will predict the next word in that
1:18
sentence and then it will take the original question and the next predicted word as an input and then predict the
1:25
next word and then the next word and so on in the end it produces is a complete
1:31
answer which almost sounds like a magic to summarize the goal of a language model is to predict a next word in a
1:39
sentence now that we have understood this fundamental let's look into some of
1:44
the topics which needs to be clarified before we dig into the actual architecture the first concept we need
Word Embeddings
1:50
to understand is word embedding machine learning models do not understand text
1:56
they understand numbers so we need to represent text as numbers let's say you
2:01
have this word King you want to numerically represent it how would you do that well you can assign a fixed
2:08
number you can have a vocabulary and you can assign just fixed static number but that will not capture the meaning of it
2:15
when you're building a language model you have to represent words in such a
2:21
way that they capture the meaning of that word one way to capture the meaning
2:27
of this word King and represent it numerically is to ask bunch of questions
2:33
for example does this person has Authority yes one do they have a tail no
2:38
horse has a tail King doesn't have a tail are they Rich yes gender minus one is male one is female and so on what we
2:46
just did is we created this Vector list of numbers which is a vector to
2:53
represent this word King similarly we can represent the word queen as well not
2:58
only that we can can represent bunch of words such as battle horse King and so
3:04
on by asking set of questions okay so for battle they don't have authority are
3:09
they an event yes do battle has tail no battle is an event it it doesn't have
3:15
tail and so on similarly horse do they have authority well we'll just say 01 if
3:21
it is King's horse maybe they have some Authority or maybe they have authority over their horse kids and so on
3:29
similarly we can represent all these words in a numeric format and then we
3:34
can take the vector of this word King and maybe we can do some mathematics
3:41
with it we can say King minus man so here I'm taking the vector of man right
3:49
which is uh this particular Vector plus woman which is this particular vector and when you do the math which is like 1
3:56
- 2 + 2 will be Z and so on you get a vector which looks
4:04
similar to Queen now this sounds like a magic we can do math with Words which is
4:09
King minus man plus woman is equal to Queen here King was represented in five
4:16
Dimensions when you look at the real life for example Google's word to wack
4:22
model it has 300 dimensions and what are all these questions by the way well we
4:28
actually don't know this has been trained through a neural network and we
4:34
have processed huge amount of text such as all the Wikipedia articles all the
4:39
books and text on internet to understand the relationship between these words and
4:45
through that neural network training back propagation we came up with this Vector the example that I gave for King
4:53
where we asked these questions Authority and so on that was just a madeup example
4:58
for building intuition for word embeddings in real life we do not know
5:05
what all these number means all we can say is these numbers are the features
5:11
for this word and they capture the meaning of this word King let's say king
5:16
is a three-dimensional embedding if you have to represent that in this 3D
5:23
embedding space you can represent it like this where x axis has three like
5:28
this three number y AIS has this eight number z-axis has this two number and so on I use three dimensions because as
5:35
humans we can view only three dimensions we can't possibly view this 300 Dimension okay but mathematically those
5:43
300 dimensions are possible models like GPT uses an embedding Vector which is
5:49
even 12,000 Dimensions okay so it's a very rich High dimensional space that we
5:55
are working with in threedimensional space you can have vectors for King king and queen that looks something like this
6:02
and if you look at this Vector which is joining king and queen you can think of
6:08
that as a gender Direction the benefit of this gender Direction Vector is that
6:14
when you have another embedding for Uncle you can add that gender Direction
6:19
and get the embedding for word Aunt similarly if you have father you can get
6:25
mother if you have man you can uh get woman and so on and that allows you to
6:30
do this amazing math such as king minus Queen plus uncle is equal to Aunt another example is you can have country
6:38
to Capital City Direction Vector which you can use to add it in this embeding
6:45
of Russia to get the embedding of Moscow you can do Russia minus Moscow plus
6:50
Delhi equal to India now the embedding that we're talking about are static embeddings wtu and glow are two popular
6:59
models which helps you get the static embedding static means fixed embedding for all
7:05
these words you may ask how these embeddings are generated well I already answered the question which is you train
7:11
a neural network model on humongous amount of text Wikipedia books and so on
7:16
to understand the relationship between the words I'm not going to go into the
7:21
mathematical details of word to you can refer to some other material on internet
7:27
I have YouTube videos for that I'm not going to go into the math of that but
7:33
just think that uh the neural network tries to understand the relationship between these words and creates these
7:39
static embeddings now the problem with static embedding is that you can have a static embedding for this word track but
7:48
based on a sentence that track can me mean different things right like here I'm saying the train will run on the
7:54
track and my package is late help me track it so the meaning of track is little different and when you have
8:00
static embedding you get into this problem where you're not able to represent this word properly based on
8:07
the context of this sentence you will see same issue here for Dish you can
8:13
have a fixed embedding but in this sentence I'm talking about rice dish if
8:19
I had a cheese dish then the embedding of dish should be a little different because the meaning of that dish word is
8:26
little different when I say rice dish versus cheese dish when you are working on predicting the next word for this
8:33
sentence you can have words such as risotto itly Mexican rice but when I say
8:39
I made an Indian rice dish call all of a sudden the probabilities of my next
8:44
words will change I will have words such as idly Biryani ke if I add one more
8:51
adjective and say I made a sweet Indian rice dish in that case again it will
8:56
change I will not have Biryani as a next word prediction I will probably have K
9:01
or Pongal to summarize to build an application like CH GPT just the static
9:08
word embeddings are not enough what you need is contextual embedding let's
Contextual Embeddings
9:13
understand contextual embedding a bit more in detail when you represent this word dish in your embedding space it is
9:21
aesthetic embedding when you say rise this maybe there is another Vector in
9:28
the same space which can accurately describe rice dis or which can correctly
9:36
capture the meaning of word rice dish which can be Roto Biryani and so on the
9:41
direction from dish to rice dish we can call it ress and when you add that ress
9:46
Vector to Dish what you get is the embedding for a rice dish there is
9:52
another vector or embedding for Indian rice dis and to go from Rice dis to
9:57
Indian rice dis you need to probably add this vector or a direction called indianness and same thing for sweet
10:04
Indian rice dish in order to generate contextual embedding what we need to do
10:10
is take the original static embedding for the word dish and have all these
10:16
other words influence that static embedding or change that static
10:22
embedding so that it can capture the meaning of all these adjectives once you
10:27
have done that and once you have a a contextual embedding for dash it won't be hard to predict the next word which
10:33
is K look at this another sentence where I'm saying D loves Dosa Dola and Millet
10:40
and so on B loves pasta and so on they both went out for a dinner and here BN
10:47
said bro we'll go to a restaurant that you like and after some time they were
10:52
in Indian restaurant now the way you predicted this word Indian was based on
10:58
this cont such as D LS all these items which are part of the Indian Cuisine also bin said
11:08
to D bro will go to a restaurant you like if bin said here instead of you if
11:14
he had said I this word will become Italian instead of Indian right also
11:22
instead of B if there was double here then also this thing will become Italian
11:29
so you can understand that this uh prediction Indian uh is influenced by
11:35
not just the few words which are prior to that word but it can be influenced by
11:41
some words which are far out in that paragraph okay to summarize the objective for this intelligent teacher
11:48
cat is to generate a contextual embedding and if you think about this
11:53
embedding space mathematically speaking what you're doing is taking the static
11:59
embedding for the word Dash and then adding the embeddings for all these
12:05
vectors ress indianness all these adjectives and getting your final
12:10
contextual embedding let's now dig into the Transformer architecture the architecture has two components encoder
Encoded Decoder
12:18
and decoder the purpose of encoder is to take the input sentence and generate the
12:24
contextual embedding for each of the words or each of the tokens in that
12:29
sentence once the contextual embedding is generated we feed that to a decoder
12:36
here and we try to predict the next word so if you're working on the next word
12:42
prediction you will predict the next word here for example it will be here when you talk about natural language
12:48
processing there are multiple tasks so one task is to predict the next word the other task would be to translate the
12:55
sentence here I'm translating the sentence from English English to Hindi in that case you will still produce the
13:03
contextual embedding from encoder you feed that to decoder and decoder will
13:10
start predicting the next word so here it will start with this fixed start token and then it will uh produce the
13:19
probability of the next word so here the probability of this word man is highest
13:25
and here you can have the entire vocabulary for example in the case of
13:31
bird you have some 30,000 words so you'll have all the words in your language and you will say okay what is
13:38
the highest probability of my next word then you put that word man into this
13:44
input okay so there are two inputs actually one is the contextual embedding
13:49
which is coming uh from here from the encoder and the other one is whatever
13:55
output you have produced so far from the previous step you feed that okay as an
14:00
input here and then it will produce the next word which is K once again you
14:06
provide key here and it produces banai So eventually it produces the entire
14:13
translated sentence all right so that's the objective of your encoder and decoder part whatever I talked about so
14:21
far I was referring to an inference stage of uh neural networks whenever you
14:28
have these uh deep learning model you have two stages one stage is the model
14:33
is not trained it's like a baby baby is not trained yet and you train them right you send them to school you train them
14:41
uh in your home at some point they become adult and they can figure things out on their own similarly a machine
14:48
learning model goes through a training phase and when it is ready it starts
14:53
working on the real world problem and that is called inference so whatever I talked about for for predicting the next
15:00
word or translating sentence I was referring to inference stage throughout the discussion we'll be referring to two
15:07
specific models called bird and GPT if you look at this architecture that's a
15:13
generic architecture for a Transformer model Transformer model is a general
15:18
concept whereas bird and GPT are specific model or specific
15:23
implementations based on Transformer architecture if you look at bird architecture it has only the encoder
15:31
part okay so only this part decoder part is missing so but will take the input
15:36
sentence it will produce the contextual embedding and that's it whereas GPT has
15:42
only decoder so it still takes the input it will produce the contextual embedding
15:47
and so on and then here it will predict the next word I mean it sounds like it
15:53
has encoder decoder both but fundamentally the architecture looks
15:58
little different for GPT but it is still based on the Transformer architecture
16:04
the way they're trained is you take all the text from Wikipedia crawl text from internet book Corpus and you train these
16:12
models when you have this article for example and you are having this sentence developing an advanced crude see if I
16:19
give you this sentence most likely you will say spacecraft or vehicle as a next word you'll not say banana right like so
16:26
probability of having banana as a next word in this sentence is very less whereas these two words have a high
16:32
probability so we as humans have read so much text so now we have learned this
16:38
art of predicting next word and same thing goes on for B and GPT where they
16:45
understand the relationship between the words the context in which they appear so let's say if B during the training
16:52
has encountered so much text and every time after this word crude if it has
16:59
seen this word spacecraft of or vehicle uh it would not have seen words like
17:05
crude chair or crude banana right that kind of words usually when it is going through training it will not see it so
17:12
it will learn to predict the high probability worse right so for word like banana probability is going to be lower
17:19
same thing for this article when you go through this kind of sentences right SI engaging both alliances and hostilities
17:27
and there will be many more artic on battles and Warriors and everywhere
17:32
after alliances there will be either hostilities or negotiations there will not be a word like chair so probability
17:39
of that will be very very low now when you go through the training you are
17:45
going through all these words right so all these words will form something called a vocabulary so for a Model A
17:53
vocabulary will look something like this now there is a difference between a word and a token so for example here playing
18:02
is a word and one of the way to tokenize is to have two token so one token is
18:08
play second token is ing okay so token wise there are two tokens but word is
18:14
just once but just for uh understanding purpose just for easy explanation you
18:21
can think of word as tokens technically they're different but you can think of words as token only okay so let's say
18:28
you have a vocabulary of all these tokens let's say 30,000 words what happens here is for
18:36
each of these words during the training it will create those static embeddings
18:42
so for word made or let's say for word and seven is the index and let's say
18:49
this is the static embedding vector and the dimension or the size see there is a
18:54
dot dot dot so what is the size of this well for bird it is 768 for GPT it's
19:02
12,228 right so based on model the dimension of your embedding Vector can
19:07
vary when you go through this training for every token in your vocabulary you
19:14
will have this static embedding and this whole table is called Static embedding
19:21
Matrix during the training it will also learn few other things such as WQ WK WV
19:28
and you are like what the hell this is well we will talk about this later but for now just remember when you train
19:35
these models they are having this static word embedding metrix which is the
19:41
static embedding for every token in your entire vocabulary as well as they're
19:46
having the special metries WQ WK WV which we'll talk about later let's have
Tokenization Positional Embeddings
19:52
a look inside the encoder and review this specific two steps so you give a
19:59
sentence to your Transformer and it will first tokenize it tokens are kind of
20:05
like words but for a word called there will be two tokens call and Ed so it
20:11
will tokenize it and there are various ways to tokenize your sentence this is one of the ways it will also add special
20:19
tokens at the end and at the beginning CLS and sep sep is for separators so if
20:25
you have two sentences between two sentences there will be a separator and CLS will be added at the beginning and
20:32
this I'm talking about bird then it will also generate token IDs so for each of
20:38
these words there will be an index into your vocabulary for example made is
20:44
2532 which means in your entire vocabulary which is just like a list made word is at position
20:54
2532 if you talk about bird it has total 30,00 , 522 tokens and GPT has around
21:03
50,000 tokens from these token ID so step number one was generate token and
21:09
token IDs then you uh get the static embedding for each of these tokens and
21:16
from where do you get it well we just saw right during the training you are generating this static word embedding
21:23
metrix so for each of the words or tokens you have
21:29
the static word embedding so in the case of bird the size of this will be 768 if
21:36
it is GPT it will be 12,000 you know that long embedding metrics so you
21:41
produce that for each of the tokens and then you will also create something
21:46
called a positional embedding now in the language the word order matters okay so
21:52
if I put made before I it will change the meaning of that sentence so the order matters and the way Transformer
21:59
works is it will process the entire input or sequence in parallel it is not
22:05
like RNN where it will process these words one by one it will process this
22:11
sequence All In Parallel now it needs to have knowledge on the order okay so for
22:18
that it uses a special technique called positional embedding where it will add a
22:26
small Vector in each of these embeddings okay so let's say this is the vector for
22:32
position number one this is the vector for position number two and so on and
22:37
when you get uh this resulting Vector this Vector will embed the knowledge of
22:45
position so this Vector will have a knowledge that this is the first word
22:50
this Vector will have a knowledge that this is the second word now how exactly that is done well there is a math behind
22:57
it I'm not going to go into the math but I'm showing you the formula from the original Transformer paper so using this
23:04
formula you are essentially uh deriving all these positional embeddings all
23:10
right so that was step two the first step was to produce the static embedding
23:16
for each of the tokens and then the second step is to add positional embedding like this is a plus sign so
23:23
here at this point what you get is this kind of position
Attention is all you need
23:29
embedding just like how my nephew needs my attention words also need attention
23:34
of surrounding words in order to produce the contextual embedding in
23:42
2017 a groundbreaking research was done when this paper attention is all you
23:48
need was published by bunch of Google researchers and that completely
23:54
transformed the landscape of AI okay and and this is the architecture that we are
24:00
talking about the architecture is taken from this attention is all you need paper so the way it works is when you
24:07
have this sentence the word Indian needs attention from Dosa Dola Etc you can say
24:15
that all these words are attending to this word Indian even the word b instead
24:22
of B in let's say here if I had Dil this will become Italian instead of Indian so
24:29
this word bin is attending to this word Indian Dosa Dola Etc is also attending
24:34
to this word Indian similarly uh words Sweet Indian rice Etc are attending to
24:43
this word dish now how much they're attending to this word well that attention weight or attention score
24:50
might be different for example sweet might be attending to this word dish by
24:55
36 person let's say Indian is attending in it by 14 person rice is attending it
25:01
by 18 person these are the adjectives which will enrich the meaning of word
25:06
dish on the other hand the word I made Etc are not enriching the meaning of
25:14
word this that much because instead of I if I had Rahul or moan or David the
25:22
meaning of this word will not change that much but instead of sweet if I have
25:27
spicy all of a sudden the embedding or the meaning of this dish changes because
25:32
as a next word I will immediately have Biryani instead of K the goal here is to
25:38
build this kind of attention weight or attention score okay for each of these
25:43
words it's a matrix because for Dish all the other words in that same sentence
25:50
are they are enriching the meaning of that word okay so for word dish let's
25:56
say sweet is attending it by 36 person Indian is attending it by 11 person and
26:03
so on and by the way I have just made up these numbers just for explanation purpose the word dish also uh attends to
26:11
that word itself right because dish itself has some meaning dish means dish right so that will also attend to itself
26:19
so for every word see right now for Dish I have all this scores for Rice you will have scores for Indian for every word
26:27
you will try to compute these attention scores and then you will use this
26:33
concept of query key and value to uh come up with the contextual embedding
26:40
now let me explain you query key and value by going over analogy let's say you're going to a library looking for a
26:47
book on quantum physics especially Quantum computation you might have this
26:52
query that hey I'm looking for this quantum physics book and this particular
26:57
person who is a librarian will use the book index so he'll go to his computer
27:04
try to search for that book or maybe he will go to this rack and locate a specific rack which has a label quantum
27:11
mechanics okay so for him the key or the index to locate that book is the label
27:19
on the rack you know in library you see like history drama science those kind of
27:24
labels or you have book description okay so based on book description the rack
27:31
label you will figure out the appropriate book so The Book Rack book
27:37
description Etc is called key and then the actual book content is your value so
27:44
let's say you pull this book okay and whatever content the actual content of that book is value let me give you
27:51
another example let's say there is a college professor who wants to write an essay on Quantum Computing and he needs
27:57
help help of bunch of students so when he talks to these students moan says
28:03
that I know linear algebra Mera says that I know quantum mechanics Bob will
28:09
say hey I know philosophy same way Kathy knows computer science so here whatever
28:16
moan mea Bob Kathy are claiming about their knowledge is called key and what
28:24
happens after that is each of these students will start writing an essay so
28:29
teacher will say okay just go and write um some bunch of paragraphs so mea moan
28:36
Kathy Bob wrote all these paragraphs which are called value and then teacher
28:42
knows that mea knows most about quantum mechanics okay so he will take 60% of
28:49
mea's content or mea's value he will take 29% of kath's value because
28:56
computer science and quantum Computing so that it's kind of related so he will
29:01
use 60% of mea's content 29% of Kathy's content to formulate that final essay on
29:09
the other hand Bob's content he will use only one person because the query and
29:15
key are not matching that much see Bob has a knowledge on philosophy but our query requires Quantum Computing so
29:22
query and key we can say they're not matching in terms of math you can think about Dot produ so let's say dot product
29:29
between query and key Vector is less let's say only one person okay but in
29:35
the other case mea query and key dot product is higher let's say 60% so you
29:40
will take 60% of mea's value which is the essay written by mea on Quantum
29:48
Computing now same way for our sentence the query for Dish is I want to know
29:54
about my modifiers okay I'm just giving you analogy by the way way the real working is little different but let's
30:01
say you are generating contextual embedding for the word dish and the query may look something like I want to
30:08
know about my modifiers right like my adjectives all these adjectives which modifies my meaning and the key will be
30:16
uh the description that each of these words are giving about themselves for example I will say I'm the subject of
30:22
the sentence made will say I indicate an action or a verb similarly sweet will say say I am an adjective describing
30:30
taste and so on so these are called keys and based on the dot product between
30:37
query and key yeah you're trying to find out you know which things are matching so if if dish wants to know about
30:44
modifiers I think these are the adjectives which modifies the meaning of word dish so the score attention score
30:52
for these will be higher whereas the tension score for these will be lower
30:58
now once you get all these attention scores you need value so each of these words will now say the value value means
31:05
uh the component that it is contributing to that query so I will say Indian will
31:12
say the style or origin is Indian sweet will say The Taste is sweet similarly all these words will have specific value
31:21
and then uh let's consider the values of only these four words I mean as such it will use values of all the words but for
31:27
simpl let's say only these four words these values by the way will be some
31:33
kind of vector we'll look into how exactly those vectors are derived but let's say these values are all these
31:39
vectors and query also has like dish also has its own Vector right like this is the static embedding so this is its
31:46
own vector and now what you do is in static embedding you add all these vectors and all these vectors you can
31:52
think about as ress indianness okay so see this is how you add all of them okay
32:00
you add all of them actually the vector of all the other words and you get the
32:06
final context of where embedding in terms of the embedding space it is like going from dish to ress indan ress and
32:14
so on so these vectors right ress indianness sweetness are these vectors
32:20
okay this is just a mathematical representation now let's look at how those vectors are built so here you have
32:27
a query for Dish okay so let me just represent it as a horizontal right this was a vertical format this is horizontal
32:34
format the same thing for each of these words or tokens you will first get their
32:40
embedding from our stating embedding Matrix okay so these are static embeddings for each of these words in
32:46
the case of bird the dimension is 768 for GPT is 12,000 something let's say
32:51
for word dish this is my embedding let's call it E7 that E7 you will multiply
32:58
with a special Matrix called WQ which will have a
33:05
dimension uh of 64 by 768 so 768 is the columns in order to perform matrix
33:12
multiplication The Columns in the first Matrix should be equal to rows in the second Matrix so this is 6 768 this is
33:19
768 the rows in The Matrix the first Matrix is 64 for bir for GPT is
33:26
different and when you do uh this kind of matrix multiplication
33:31
you will get uh this quy Vector okay so you will multiply this row with this
33:38
column okay so you multiply 50 with this 0.9
33:44
minus5 with 1.07 65 with this and then you add them
33:50
all up you put them here then you take the second row multiply 23 with this
33:56
minus 71 with this 1.58 with this and you put that here and so on okay so this
34:03
is how you build a query Vector now WQ here knows how to encode query of a
34:10
token for attention computation when we train the model we already got the WQ
34:17
and WQ after the training is done it it doesn't change okay after you do that
34:24
training sometime it is referred as pre-training on huge amount of data you
34:30
build this WQ Matrix which doesn't change okay so for a train model uh this WQ will not change you multiply that
34:37
with specific embedding E7 let's say this is a positional embedding you get
34:43
Q7 which is the query Vector for the word dish and you repeat the same
34:49
process for all the words okay so how you have Q7 for dish for Rice you will have q6 Indian you have uh Q5 and so on
34:58
to summarize WQ here knows how to encode query of a token for attention
35:05
computation and remember in one of the previous slides I said that when the model is strained it will have static
35:12
embedding metrix but it will also have this WQ WK WV and that is what I was
35:18
referring to okay so we just talked about WQ here the question now is during
35:23
the training how exactly we get WQ WK WV well we take this Transformer
35:29
architecture and we train it on huge amount of data so we take all the Wikipedia text and we generate this kind
35:36
of X and Y pairs okay so you don't have to manually label it this is called uh
35:42
self-supervised data set uh you don't need a person to label it because you can just split a sentence you can have a
35:48
sentence and the next word is your y okay so this is your X this is your y you feed X as an input and when the
35:56
model is not train TR it will not predict right things it will make error so let's say for this it produce Mexican
36:03
which is your why hat okay it's a predicted value your actual value is Indian so that is why you calculate
36:09
error and then you back propagate that error through back propagation and chain
36:15
rule partial derivative and so on folks you need to have understanding of how
36:20
back propagation Works what is a chain rule you need to know all those deep learning fundamentals okay I have
36:26
covered that in other modules if you're part of my courses or boot camp you
36:32
would have seen those if you're watching it from YouTube Again YouTube has uh these kind of tutorials my channel has
36:38
these tutorials so you need to know how the back propagation Works essentially
36:44
you are feeding this data set you're Computing the error and you're back propagating it throughout this
36:49
architecture and during that back propagation when let's say you train this on millions and millions of
36:55
sentences that is the time when uh this WQ WK WV will be finalized inside this
37:04
model architecture now going back for Dish query we computed this particular
37:11
query Vector next step is to compute the key vectors okay so I gave this kind of
37:18
analogy description to uh get you an intuitive understanding but in reality
37:24
these will be the vectors so let's see how those vectors are formed so here I'm
37:30
taking the first token I and the keys look something like this okay so here
37:36
you will take the positional embedding the static embedding for the word I and
37:41
you will multiply that with another magical Matrix WK once again WK after
37:48
your pre-training after that model is trained it is fixed so you take that Matrix and you uh figure out your K1
37:57
okay here WK knows how to encode key of a token for the attention computation
38:05
then you go to the next word compute K2 next word compute K3 you do that for all
38:11
the words so now for all these words we have these key vectors okay so you have
38:18
Q7 uh query Vector you have key vectors and you take the dot product between
38:24
these two okay so q1 K1 Dot Q7 okay so if you take these dot product between
38:31
these two vectors you'll get some number right like 3.33 57 101 whatever that number is it
38:40
it's a single number you will get that for all the tokens okay and then you let
38:48
it pass through a soft Max function from Deep planning fundamentals you should know about softmax softmax will convert
38:55
bunch of values into probability distrib ution so that when you add all these values it will be one so soft Max is
39:02
converting all these discrete values into probability distribution so that you can express them as percentages and
39:10
the sum of all these percentage will be one mathematically you can represent this operation as soft Max between q and
39:18
KT now KT is K transpose okay so here Q
39:24
was a vector but if you talk about let's say this K right so K is k1 K2 K3 so
39:30
it's not just one vector actually it's like bunch of vectors so this can be thought of as a matrix and to multiply
39:37
that you need to do a transpose see if you are multiplying Q7 with K1 like a
39:42
single Vector you don't need to do transpose but when you have Matrix you need to do transpose okay so we'll use
39:48
this formula later on in the final attention formula but for now just remember that there is this kind of
39:54
formula as a Next Step once you have comp Ed these attention scores or
39:59
attention weights you need to find the value Vector right so this was a
40:05
descriptive uh understanding of value Vector but the way value vectors are derived is similar for each of the
40:12
tokens you get positional embedding static embedding then you multiply that
40:18
with another Vector called WV you get V1 and here WV knows how to encode value of
40:26
a token for attention computation okay so you do that for all the words so V1
40:32
V2 V3 V4 V7 and so on okay so for all these words you will uh get their values
40:41
and you multiply that with the weight so you will have more component from this
40:47
V4 Vector because it's like 36 person but the component that you will use from
40:52
V1 will be very less 7 person okay so see the sweetness you're taking
41:00
36% uh here I don't have things in order but you essentially add all the vectors
41:06
okay so you just add all of this everything okay so here I'm not showing everything but you kind of get an idea
41:13
so from static embedding you go all the way to context aware embedding here's
41:19
the mathematical formula for attention qk V where DK is a dimension of a key
41:26
vector in case of GPT this is 128 so what they do is they take um the entire 12
41:36
228 Dimension right for GPT the dimension of the contextual embeddings
41:42
is 12 228 and you divide it by the number of attention heads I think for
41:48
GPT is 96 and that's how you get 128 I will explain this 96 a little later but
41:54
there is a way to derive this number 128 so you do division by square root of
42:02
that just for numerical stability you don't want this dot products to become very high okay so to bring down that
42:08
number we do kind of scaling here and you do soft Max and you multiply that
42:14
with this value V so far what we talked about is a single attention block
42:20
actually there are multiple attention blocks so that's what we'll cover next let's understand what is multi head
Multi-Head Attention
42:28
attention so far we have seen this picture where you take positional
42:34
embedding for each of the words in your input sequence you let it go through
42:39
attention head which is basically taking this WQ WK
42:45
WV and coming up with context our aware embedding so that whole portion is
42:51
called one attention head in reality you have multiple attention heads okay so
42:57
you have multiple attention heads each of these heads are producing their own
43:02
context aware embedding which you will add them up all together to get the
43:09
final context aware embedding now what is the purpose of this multiple attention heads one attention head will
43:16
be working on adjectives okay so for the word dish sweet Indian rice Etc are
43:22
adjectives the second attention head might be working on on a verb okay so
43:29
how this verb made uh affects the contextual embedding of the word dish
43:34
the third attention block might be looking at pronoun so you can think of
43:40
this as looking at different aspects of a language or different aspects of that
43:47
context okay for the other sentence the first attention head might be looking at a cultural context such as Dosa Dola
43:54
Millet bread are all Indian Delicacies whereas the second attention head might be looking at the pronoun where instead
44:02
of the and B if I exchange the order of these two uh here you will have Italian
44:07
similarly instead of you if I say I again here you will have a different
44:13
word so there is a pronoun context the third attention uh head might be looking at action and timing you know you're
44:20
driving 20 minutes Drive Etc so the purpose of multi- attention heads is to
44:26
allow the model to focus on different aspects or
44:32
different types of relationships between tokens in a language when you have
44:37
multiple tokens there is a different type of relationship between these tokens such as semantic positional
44:45
syntactic uh simultaneously uh enriching the contextual understanding of each uh
44:53
token so I want you to read this sentence again uh I hope hope you get an idea it is basically looking at
44:59
different aspects or different relationship between the tokens to enrich the
45:05
contextual understanding of each token so here in this particular architecture
45:12
diagram see first we produced this uh static embedding then we added this
45:17
positional encoding right so you got positional encoding here uh you ignore this normalization part for now
45:23
normalization is simple actually it's like uh normal izing it to Value which is zero mean and one standard deviation
45:31
and then looking at v k q kind of metrix to uh use multi-headed attention to
45:40
derive ec1 ec2 these individual uh contextual embedding and
45:46
you add all of them up to produce your final context of our embedding which
45:53
will come here and by the way this is a residual connection uh if you know about
45:58
deep learning you will have uh this um residual connection that helps you uh
46:06
with a smooth gradient flow after this block the next block is feed forward
46:12
Network so you'll ask me okay I already have context over embedding now why do I
46:18
need this feed forward Network well the thing is you don't have your final
46:23
context aware embedding yet so here at this point the embeddings are enriched but they are
46:30
not still fully furnished yet you have to let it go through this feed forward
46:36
Network so what happens is you passed your positional embedding through bunch
46:41
of attention heads and you got this enriched contextual embedding that will
46:47
go through a fully connected neural network layer okay so here the input
46:53
neurons will be same number of uh elements as this embedding so in case of
46:59
bir let's say this will be 768 for GPT it will be 12
47:05
228 and then in the hidden layer you can have uh n n number of neurons and in the
47:11
output layer again you'll have same as this one because this input and this output will have a same size so if this
47:18
is 768 this will also be 768 okay so you let it go through this feed forward
47:24
Network and the resulting embedding that you get is even more enrich it's like a
47:30
more furnac product now this neural network weights you know this will have
47:36
a lot of weights and parameters those weights and parameters are set once again during that training process so
47:43
when you're going through this XY pairs right your training pairs you might have hundreds and thousands of these
47:50
sentences when you're training that Network during that training look at this feed forward Network you know
47:55
during back propagation those weights are getting adjusted and it will help you refine your sentence
48:03
further now once you get enriched embedding you will add that into your
48:08
original embedding and you get the final now it is final now it's a final
48:14
contextually Rich embedding so the purpose of feed forward network is it
48:19
will enrich each token embedding by applying nonlinear transformation
48:25
because in the attention head you are applying linear transformation here you
48:30
get an opportunity to apply nonlinear transformation independently enabling the model to learn complex patterns and
48:38
higher order features Beyond just the contextual relationship see multi-head
48:43
attention is just capturing those contextual relationships how these words are related to each other but language
48:49
is nonlinear it's not just the relationship right there are like some nuances nonlinearity complexity all of
48:56
that can be captured by this fully connected layer or feed forward Network
49:01
so to better visualize each of the words in your sentence let's say you have I made dish every word will go through
49:09
positional embedding and every positional embedding goes through multiple attention head so the embedding
49:15
for I will go through all the heads okay so in GPT if you have 96 heads it will
49:21
go through all those 96 heads similarly made will also go through 96 heads and
49:26
this this is happening in parallel it's not like you process I first and made no
49:31
all of these things are happening in parallel and each of these uh vectors will also go through the feed forward
49:38
Network parall at the same time right so the same network is available for each of these words and you get all these
49:45
contextually enriched embeddings okay so that comes here so after feed forward
49:51
Network here at this point you get all these m Bings okay and then you have
49:59
this uh plus sign and normalization so normalization layer by the way this Norm
50:05
is uh just it's ensuring that you have stable learning improving the gradient
50:10
flow if you have deep learning fundamentals you will understand what I mean uh in machine learning generally
50:16
when you have all these wide range of values if you normalize them let's say you normalize them to zero and one you
50:23
get better control over your training now you also notice this anx layers so
50:29
anx layers is basically for B let's say if you have a b base model you have 12
50:36
such layers okay so this is a Transformer block so you kind of repeat so you have one block then after that
50:42
you have another block so in case of BT base model you have 12 layers B large you have 24 layers in case of GPT again
50:51
there will be different number of layers so that's what this NX layers means all right f finally we are done with
50:58
understanding encoder I just want to summarize we had an input sequence we generated a static embedding here then
51:06
here we generated a positional embedding then we have one Transformer block or NX layer where we first normalize we use
51:14
vkv uh to compute attention score or attention weight we have multiple such
51:20
heads and then you go through normalization you have feed forward Network you kind of ADD remember like
51:27
you have original embedding and then you add that output and you get the final contextual embedding you normalize it
51:34
and here at this point you are getting a final contextual uh very enriched embedding we
Decoder
51:41
have covered most of the Transformer architecture decoder is not going to take uh much time so let's spend few
51:48
minutes understanding decoder so the output of encoder is a contextual
51:54
embedding or context Rich embedding which you give it as an input to decoder
52:00
and decoder will produce the next word if you're working on next word prediction if you're working on language
52:07
translation it will uh start with this special token called start and then it
52:12
will produce May then another work here banay and so on okay so that's a goal of
52:18
a decoder now here you will notice one thing which is called
52:24
multi-headed cross attention okay so let's understand what exactly is cross
52:29
attention let's say you have this sentence I made kir which you want to translate into Hindi here you will have
52:36
key vectors and value vectors as we have discussed before but the query Vector will be little different so query Vector
52:42
will be start and it will be like I'm starting to generate translation what part of the input should I focus on and
52:49
then when you have next word which is man which is the first word in your translation it will be like I generated
52:57
the subject what is the subject okay then you will have here I generated the
53:02
subject and object help me complate the sentence with a verb form so the query
53:08
part is little different see in the previous example you had only one sentence so I made key so you'll
53:15
generate a query from made let's say and you will have key and value from same
53:20
sentence in case of language translation it's little different uh so here we need to use cross cross attention why cross
53:28
attention because query you are using it from the translated sentence in Hindi
53:34
whereas key and values are being used from the original sentence in English so
53:40
that is why it's called cross attention in the diagram you can see here the V and K values are coming from your
53:47
encoder encoder has processed this im here okay so V and K are coming from
53:54
encod see this is the arrow whereas Q is coming from the decoder itself right so
54:01
you know that Hindi sequence will be produced here so man K banai Etc so that
54:07
query part is coming from here that is why it is called cross attention in the
54:13
case of B we all know that there is only encoder part decoder part is not there in case of GPT the Transformer
54:20
architecture is little different okay so that's the encoder part remaining part we have already understood now let me uh
54:27
show one nice tool which can visually show you this architecture someone has built this nice visualization tool you
54:34
can go to Pol club. github.io Transformer explainer and here you can look into different examples right so
54:41
for example let's look at this sentence as the space ship was approaching the it
54:47
will try to autocomplete that word and say station right now each of these
54:53
words has the space ship first you had this Dropout layer so we talking about
55:00
Transformer here so it will have a Dropout layer the architecture is little
55:05
customized compared to the base Transformer architecture then you have this residual connection okay so
55:11
residual connection will take you from here to here if you talk about embeddings you have token embeddings for
55:18
each of these right see 768 is the size then you have positional embedding okay
55:24
you add all this position and you get final Vector this is a positional embedding of a sentence and
55:31
you have residual connection then you have q KV computation so if you look at
55:37
this particular block here qkv it will kind of visually show you how you
55:43
compute q k v and get all those three vectors right like query Vector key
55:50
vector and value vector and then uh you have this output which you feed it to
55:57
MLP is your feed forward neural network that we talked about okay so feed forward neural network then you again
56:03
have a residual block and then you have layer normalization and this is one Transformer block you have multiple of
56:10
them like 11 okay so that Annex layer that I was referring to is one block so
56:16
you have repeated blocks and in the end you get this kind of soft Max probability see the probability here is
56:23
a station okay so just play with this particular Tool uh to get a better
56:29
understanding of this thing and I want to give credits uh to this amazing
56:34
Channel called 3 blue one brown so if you go to YouTube and type in
56:40
Transformer explain 3 blue one brown you will find all these videos so I want you
56:45
to watch from video number 5678 onwards he will have more videos as well uh
56:52
especially these three video dl5 dl6 dl7 these three videos you must watch it
56:59
will enhance your understanding further I myself learned a lot from this channel
57:06
so due credits to three blue one brown all right that's it folks so that's that
57:11
was about Transformer I know it was a long discussion there were many topics that we covered but hopefully your
57:18
understanding is clear if you have any question please feel free to ask
57:23
[Music] a [Music]
